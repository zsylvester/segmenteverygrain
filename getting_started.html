<!DOCTYPE html>

<html lang="en" data-content_root="./">
  <head>
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" /><meta name="viewport" content="width=device-width, initial-scale=1" />

    <title>Getting started &#8212; segmenteverygrain 0.2.1 documentation</title>
    <link rel="stylesheet" type="text/css" href="_static/pygments.css?v=5ecbeea2" />
    <link rel="stylesheet" type="text/css" href="_static/basic.css?v=686e5160" />
    <link rel="stylesheet" type="text/css" href="_static/alabaster.css?v=27fed22d" />
    <script src="_static/documentation_options.js?v=37f418d5"></script>
    <script src="_static/doctools.js?v=9bcbadda"></script>
    <script src="_static/sphinx_highlight.js?v=dc90522c"></script>
    <link rel="index" title="Index" href="genindex.html" />
    <link rel="search" title="Search" href="search.html" />
    <link rel="next" title="API Reference" href="api_reference.html" />
    <link rel="prev" title="segmenteverygrain" href="index.html" />
   
  <link rel="stylesheet" href="_static/custom.css" type="text/css" />
  

  
  

  </head><body>
  

    <div class="document">
      <div class="documentwrapper">
        <div class="bodywrapper">
          

          <div class="body" role="main">
            
  <section id="getting-started">
<h1>Getting started<a class="headerlink" href="#getting-started" title="Link to this heading">¶</a></h1>
<div class="toctree-wrapper compound">
</div>
<p>The best way to use the <cite>segmenteverygrain</cite> package is to run the <a class="reference external" href="https://github.com/zsylvester/segmenteverygrain/blob/main/notebooks/Segment_every_grain.ipynb">Segment_every_grain.ipynb</a> notebook.</p>
<p>The notebook goes through the steps of loading the models, running the segmentation, interactively updating the result, and saving the grain data and the mask. The text below summarizes the steps that you need to take to run the segmentation.</p>
<section id="loading-the-models">
<h2>Loading the models<a class="headerlink" href="#loading-the-models" title="Link to this heading">¶</a></h2>
<p>To load the U-Net model, you can use the ‘load_model’ function from Keras. The U-Net model is saved in the ‘seg_model.keras’ file.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span><span class="w"> </span><span class="nn">segmenteverygrain</span><span class="w"> </span><span class="k">as</span><span class="w"> </span><span class="nn">seg</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">keras.saving</span><span class="w"> </span><span class="kn">import</span> <span class="n">load_model</span>
<span class="n">model</span> <span class="o">=</span> <span class="n">load_model</span><span class="p">(</span><span class="s2">&quot;seg_model.keras&quot;</span><span class="p">,</span> <span class="n">custom_objects</span><span class="o">=</span><span class="p">{</span><span class="s1">&#39;weighted_crossentropy&#39;</span><span class="p">:</span> <span class="n">seg</span><span class="o">.</span><span class="n">weighted_crossentropy</span><span class="p">})</span>
</pre></div>
</div>
<p>This assumes that you are using Keras 3 and ‘seg_model.keras’ was saved using Keras 3. Older models created with a <code class="docutils literal notranslate"><span class="pre">segmenteverygrain</span></code> version that was based on Keras 2 do not work with with the latest version of the package.</p>
<p>The SAM 2.1 model can be downloaded from this <a class="reference external" href="https://dl.fbaipublicfiles.com/segment_anything_2/092424/sam2.1_hiera_large.pt">link</a>. You can also download it programmatically:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span><span class="w"> </span><span class="nn">os</span>
<span class="kn">import</span><span class="w"> </span><span class="nn">urllib.request</span>

<span class="k">if</span> <span class="ow">not</span> <span class="n">os</span><span class="o">.</span><span class="n">path</span><span class="o">.</span><span class="n">exists</span><span class="p">(</span><span class="s2">&quot;./models/sam2.1_hiera_large.pt&quot;</span><span class="p">):</span>
    <span class="n">url</span> <span class="o">=</span> <span class="s2">&quot;https://dl.fbaipublicfiles.com/segment_anything_2/092424/sam2.1_hiera_large.pt&quot;</span>
    <span class="n">urllib</span><span class="o">.</span><span class="n">request</span><span class="o">.</span><span class="n">urlretrieve</span><span class="p">(</span><span class="n">url</span><span class="p">,</span> <span class="s2">&quot;./models/sam2.1_hiera_large.pt&quot;</span><span class="p">)</span>
</pre></div>
</div>
</section>
<section id="running-the-segmentation">
<h2>Running the segmentation<a class="headerlink" href="#running-the-segmentation" title="Link to this heading">¶</a></h2>
<p>To run the U-Net segmentation on an image and label the grains in the U-Net output:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span><span class="w"> </span><span class="nn">numpy</span><span class="w"> </span><span class="k">as</span><span class="w"> </span><span class="nn">np</span>
<span class="kn">import</span><span class="w"> </span><span class="nn">matplotlib.pyplot</span><span class="w"> </span><span class="k">as</span><span class="w"> </span><span class="nn">plt</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">keras.utils</span><span class="w"> </span><span class="kn">import</span> <span class="n">load_img</span>

<span class="c1"># Load your image</span>
<span class="n">fname</span> <span class="o">=</span> <span class="s2">&quot;path/to/your/image.jpg&quot;</span>
<span class="n">image</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">(</span><span class="n">load_img</span><span class="p">(</span><span class="n">fname</span><span class="p">))</span>

<span class="c1"># Run U-Net prediction</span>
<span class="n">image_pred</span> <span class="o">=</span> <span class="n">seg</span><span class="o">.</span><span class="n">predict_image</span><span class="p">(</span><span class="n">image</span><span class="p">,</span> <span class="n">model</span><span class="p">,</span> <span class="n">I</span><span class="o">=</span><span class="mi">256</span><span class="p">)</span>
<span class="n">labels</span><span class="p">,</span> <span class="n">coords</span> <span class="o">=</span> <span class="n">seg</span><span class="o">.</span><span class="n">label_grains</span><span class="p">(</span><span class="n">image</span><span class="p">,</span> <span class="n">image_pred</span><span class="p">,</span> <span class="n">dbs_max_dist</span><span class="o">=</span><span class="mf">20.0</span><span class="p">)</span>
</pre></div>
</div>
<p>The input image should not be much larger than ~2000x3000 pixels, in part to avoid long running times; it is supposed to be a numpy array with 3 channels (RGB).
Grains should be well defined in the image and not too small (e.g., only a few pixels in size).</p>
</section>
<section id="quality-control-of-u-net-prediction">
<h2>Quality control of U-Net prediction<a class="headerlink" href="#quality-control-of-u-net-prediction" title="Link to this heading">¶</a></h2>
<p>The U-Net prediction should be QC-d before running the SAM segmentation:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">plt</span><span class="o">.</span><span class="n">figure</span><span class="p">(</span><span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">15</span><span class="p">,</span><span class="mi">10</span><span class="p">))</span>
<span class="n">plt</span><span class="o">.</span><span class="n">imshow</span><span class="p">(</span><span class="n">image_pred</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">scatter</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">(</span><span class="n">coords</span><span class="p">)[:,</span><span class="mi">0</span><span class="p">],</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">(</span><span class="n">coords</span><span class="p">)[:,</span><span class="mi">1</span><span class="p">],</span> <span class="n">c</span><span class="o">=</span><span class="s1">&#39;k&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">xticks</span><span class="p">([])</span>
<span class="n">plt</span><span class="o">.</span><span class="n">yticks</span><span class="p">([])</span>
</pre></div>
</div>
<p>The black dots in the figure represent the SAM prompts that will be used for grain segmentation. If the U-Net segmentation is of low quality, the base model can be (and should be) finetuned using the steps outlined <a class="reference internal" href="#finetuning-the-u-net-model"><span class="std std-ref">below</span></a>.</p>
</section>
<section id="sam-segmentation">
<h2>SAM segmentation<a class="headerlink" href="#sam-segmentation" title="Link to this heading">¶</a></h2>
<p>Here is an example showing how to run the SAM segmentation on an image, using the outputs from the U-Net model:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span><span class="w"> </span><span class="nn">torch</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">sam2.build_sam</span><span class="w"> </span><span class="kn">import</span> <span class="n">build_sam2</span>
<span class="c1"># Auto-detect device: CUDA for NVIDIA, MPS for Apple Silicon, CPU as fallback</span>
<span class="n">device</span> <span class="o">=</span> <span class="s2">&quot;cuda&quot;</span> <span class="k">if</span> <span class="n">torch</span><span class="o">.</span><span class="n">cuda</span><span class="o">.</span><span class="n">is_available</span><span class="p">()</span> <span class="k">else</span> <span class="s2">&quot;mps&quot;</span> <span class="k">if</span> <span class="n">torch</span><span class="o">.</span><span class="n">backends</span><span class="o">.</span><span class="n">mps</span><span class="o">.</span><span class="n">is_available</span><span class="p">()</span> <span class="k">else</span> <span class="s2">&quot;cpu&quot;</span>
<span class="n">sam</span> <span class="o">=</span> <span class="n">build_sam2</span><span class="p">(</span><span class="s2">&quot;configs/sam2.1/sam2.1_hiera_l.yaml&quot;</span><span class="p">,</span> <span class="s2">&quot;sam2.1_hiera_large.pt&quot;</span><span class="p">,</span> <span class="n">device</span><span class="o">=</span><span class="n">device</span><span class="p">)</span> <span class="c1"># load the SAM 2.1 model</span>
<span class="n">all_grains</span><span class="p">,</span> <span class="n">labels</span><span class="p">,</span> <span class="n">mask_all</span><span class="p">,</span> <span class="n">grain_data</span><span class="p">,</span> <span class="n">fig</span><span class="p">,</span> <span class="n">ax</span> <span class="o">=</span> <span class="n">seg</span><span class="o">.</span><span class="n">sam_segmentation</span><span class="p">(</span><span class="n">sam</span><span class="p">,</span> <span class="n">image</span><span class="p">,</span> <span class="n">image_pred</span><span class="p">,</span> <span class="n">coords</span><span class="p">,</span> <span class="n">labels</span><span class="p">,</span> <span class="n">min_area</span><span class="o">=</span><span class="mf">400.0</span><span class="p">,</span> <span class="n">plot_image</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">remove_edge_grains</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> <span class="n">remove_large_objects</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>
</pre></div>
</div>
<p>The <code class="docutils literal notranslate"><span class="pre">all_grains</span></code> list contains shapely polygons of the grains detected in the image. <code class="docutils literal notranslate"><span class="pre">labels</span></code> is an image that contains the labels of the grains.
<code class="docutils literal notranslate"><span class="pre">grain_data</span></code> is a pandas dataframe with a number of grain parameters.</p>
</section>
<section id="interactive-editing-of-results">
<h2>Interactive editing of results<a class="headerlink" href="#interactive-editing-of-results" title="Link to this heading">¶</a></h2>
<p>After the initial segmentation, you can interactively edit the results using the <code class="docutils literal notranslate"><span class="pre">GrainPlot</span></code> class from the <code class="docutils literal notranslate"><span class="pre">interactions</span></code> module. This provides a modern, interactive interface for deleting, merging, and adding grains.</p>
<p>First, convert the polygons to <code class="docutils literal notranslate"><span class="pre">Grain</span></code> objects and set up the SAM predictor:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span><span class="w"> </span><span class="nn">segmenteverygrain.interactions</span><span class="w"> </span><span class="k">as</span><span class="w"> </span><span class="nn">si</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">sam2.sam2_image_predictor</span><span class="w"> </span><span class="kn">import</span> <span class="n">SAM2ImagePredictor</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">tqdm</span><span class="w"> </span><span class="kn">import</span> <span class="n">tqdm</span>

<span class="c1"># Convert polygons to Grain objects</span>
<span class="n">grains</span> <span class="o">=</span> <span class="n">si</span><span class="o">.</span><span class="n">polygons_to_grains</span><span class="p">(</span><span class="n">all_grains</span><span class="p">,</span> <span class="n">image</span><span class="o">=</span><span class="n">image</span><span class="p">)</span>
<span class="k">for</span> <span class="n">g</span> <span class="ow">in</span> <span class="n">tqdm</span><span class="p">(</span><span class="n">grains</span><span class="p">,</span> <span class="n">desc</span><span class="o">=</span><span class="s1">&#39;Measuring detected grains&#39;</span><span class="p">):</span>
    <span class="n">g</span><span class="o">.</span><span class="n">measure</span><span class="p">()</span>

<span class="c1"># Set up SAM 2.1 predictor for adding new grains</span>
<span class="n">predictor</span> <span class="o">=</span> <span class="n">SAM2ImagePredictor</span><span class="p">(</span><span class="n">sam</span><span class="p">)</span>
<span class="n">predictor</span><span class="o">.</span><span class="n">set_image</span><span class="p">(</span><span class="n">image</span><span class="p">)</span>
</pre></div>
</div>
<p>Then create the interactive plot:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">plot</span> <span class="o">=</span> <span class="n">si</span><span class="o">.</span><span class="n">GrainPlot</span><span class="p">(</span>
    <span class="n">grains</span><span class="p">,</span>
    <span class="n">image</span><span class="o">=</span><span class="n">image</span><span class="p">,</span>
    <span class="n">predictor</span><span class="o">=</span><span class="n">predictor</span><span class="p">,</span>
    <span class="n">blit</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span>                      <span class="c1"># Use blitting for faster rendering</span>
    <span class="n">color_palette</span><span class="o">=</span><span class="s1">&#39;tab20b&#39;</span><span class="p">,</span>         <span class="c1"># Matplotlib colormap for grain colors</span>
    <span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">12</span><span class="p">,</span> <span class="mi">8</span><span class="p">),</span>                <span class="c1"># Figure size in inches</span>
    <span class="n">scale_m</span><span class="o">=</span><span class="mi">500</span><span class="o">*</span><span class="mf">1e-6</span><span class="p">,</span>               <span class="c1"># Length of scale bar in meters (for unit conversion)</span>
    <span class="n">px_per_m</span><span class="o">=</span><span class="mf">1.0</span><span class="p">,</span>                   <span class="c1"># Pixels per meter (will be updated if scale bar is drawn)</span>
<span class="p">)</span>
<span class="n">plot</span><span class="o">.</span><span class="n">activate</span><span class="p">()</span>
</pre></div>
</div>
<p>Interactive controls (also shown in the figure title bar):</p>
<p><strong>Mouse controls:</strong></p>
<ul class="simple">
<li><p><strong>Left-click on existing grain</strong>: Select/unselect the grain</p></li>
<li><p><strong>Left-click in grain-free area</strong>: Place foreground prompt for instant grain creation (auto-create)</p></li>
<li><p><strong>Alt + Left-click</strong>: Place foreground prompt for multi-prompt grain creation (hold Alt, click multiple times, release Alt to create)</p></li>
<li><p><strong>Alt + Right-click</strong>: Place background prompt for multi-prompt creation</p></li>
<li><p><strong>Shift + Left-drag</strong>: Draw a scale bar line (red line) for unit conversion</p></li>
<li><p><strong>Middle-click</strong> or <strong>Shift + Left-click on grain</strong>: Show grain measurement info</p></li>
</ul>
<p><strong>Keyboard controls:</strong></p>
<ul class="simple">
<li><p><strong>d</strong> or <strong>Delete</strong>: Delete selected (highlighted) grains</p></li>
<li><p><strong>m</strong>: Merge selected grains (must be touching)</p></li>
<li><p><strong>z</strong>: Undo (delete the most recently created grain)</p></li>
<li><p><strong>Ctrl</strong> (hold): Temporarily hide all grain masks</p></li>
<li><p><strong>Esc</strong>: Remove all prompts and unselect all grains</p></li>
<li><p><strong>c</strong>: Create grain from existing prompts (alternative to auto-create)</p></li>
</ul>
<p>After editing, retrieve the updated grains and deactivate the interactive features:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">grains</span> <span class="o">=</span> <span class="n">plot</span><span class="o">.</span><span class="n">get_grains</span><span class="p">()</span>  <span class="c1"># Get the edited list of grains</span>
<span class="n">plot</span><span class="o">.</span><span class="n">deactivate</span><span class="p">()</span>           <span class="c1"># Turn off interactive features</span>

<span class="c1"># Optionally draw the major and minor axes on each grain</span>
<span class="n">plot</span><span class="o">.</span><span class="n">draw_axes</span><span class="p">()</span>
</pre></div>
</div>
</section>
<section id="scale-bar-and-unit-conversion">
<h2>Scale bar and unit conversion<a class="headerlink" href="#scale-bar-and-unit-conversion" title="Link to this heading">¶</a></h2>
<p>To convert measurements from pixels to real-world units, you can either:</p>
<ol class="arabic simple">
<li><p><strong>Specify the scale when creating the plot</strong> using the <code class="docutils literal notranslate"><span class="pre">px_per_m</span></code> parameter (pixels per meter).</p></li>
<li><p><strong>Draw a scale bar interactively</strong>: Hold <code class="docutils literal notranslate"><span class="pre">Shift</span></code> and drag the mouse to draw a line on a known reference object. The <code class="docutils literal notranslate"><span class="pre">scale_m</span></code> parameter specifies the real-world length of this reference object in meters.</p></li>
</ol>
<p>After drawing a scale bar, the pixel-to-meter conversion is automatically updated:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="c1"># Retrieve the scale after drawing a scale bar</span>
<span class="n">px_per_m</span> <span class="o">=</span> <span class="n">plot</span><span class="o">.</span><span class="n">px_per_m</span>
</pre></div>
</div>
</section>
<section id="grain-size-analysis">
<h2>Grain size analysis<a class="headerlink" href="#grain-size-analysis" title="Link to this heading">¶</a></h2>
<p>Generate a summary dataframe and histogram of grain measurements:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="c1"># Get summary dataframe with all grain measurements</span>
<span class="n">summary</span> <span class="o">=</span> <span class="n">si</span><span class="o">.</span><span class="n">get_summary</span><span class="p">(</span><span class="n">grains</span><span class="p">,</span> <span class="n">px_per_m</span><span class="o">=</span><span class="n">plot</span><span class="o">.</span><span class="n">px_per_m</span><span class="p">)</span>

<span class="c1"># Create histogram of major and minor axis lengths</span>
<span class="n">hist</span> <span class="o">=</span> <span class="n">seg</span><span class="o">.</span><span class="n">plot_histogram_of_axis_lengths</span><span class="p">(</span>
    <span class="n">summary</span><span class="p">[</span><span class="s1">&#39;major_axis_length&#39;</span><span class="p">]</span> <span class="o">*</span> <span class="mi">1000</span><span class="p">,</span>  <span class="c1"># Convert to mm</span>
    <span class="n">summary</span><span class="p">[</span><span class="s1">&#39;minor_axis_length&#39;</span><span class="p">]</span> <span class="o">*</span> <span class="mi">1000</span><span class="p">,</span>
    <span class="n">binsize</span><span class="o">=</span><span class="mf">0.25</span>
<span class="p">)</span>
</pre></div>
</div>
</section>
<section id="saving-results">
<h2>Saving results<a class="headerlink" href="#saving-results" title="Link to this heading">¶</a></h2>
<p>The <code class="docutils literal notranslate"><span class="pre">interactions</span></code> module provides convenient functions for saving all results:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">out_fn</span> <span class="o">=</span> <span class="s2">&quot;./examples/output/my_image&quot;</span>  <span class="c1"># Base filename (without extension)</span>

<span class="c1"># Save grain shapes as GeoJSON</span>
<span class="n">si</span><span class="o">.</span><span class="n">save_grains</span><span class="p">(</span><span class="n">out_fn</span> <span class="o">+</span> <span class="s1">&#39;_grains.geojson&#39;</span><span class="p">,</span> <span class="n">grains</span><span class="p">)</span>

<span class="c1"># Save the plot with grain overlays</span>
<span class="n">plot</span><span class="o">.</span><span class="n">savefig</span><span class="p">(</span><span class="n">out_fn</span> <span class="o">+</span> <span class="s1">&#39;_grains.jpg&#39;</span><span class="p">)</span>

<span class="c1"># Save grain measurements as CSV</span>
<span class="n">summary</span> <span class="o">=</span> <span class="n">si</span><span class="o">.</span><span class="n">save_summary</span><span class="p">(</span><span class="n">out_fn</span> <span class="o">+</span> <span class="s1">&#39;_summary.csv&#39;</span><span class="p">,</span> <span class="n">grains</span><span class="p">,</span> <span class="n">px_per_m</span><span class="o">=</span><span class="n">plot</span><span class="o">.</span><span class="n">px_per_m</span><span class="p">)</span>

<span class="c1"># Save histogram as image</span>
<span class="n">si</span><span class="o">.</span><span class="n">save_histogram</span><span class="p">(</span><span class="n">out_fn</span> <span class="o">+</span> <span class="s1">&#39;_summary.jpg&#39;</span><span class="p">,</span> <span class="n">summary</span><span class="o">=</span><span class="n">summary</span><span class="p">)</span>

<span class="c1"># Save binary mask for training (0-1 values)</span>
<span class="n">si</span><span class="o">.</span><span class="n">save_mask</span><span class="p">(</span><span class="n">out_fn</span> <span class="o">+</span> <span class="s1">&#39;_mask.png&#39;</span><span class="p">,</span> <span class="n">grains</span><span class="p">,</span> <span class="n">image</span><span class="p">,</span> <span class="n">scale</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>

<span class="c1"># Save human-readable mask (0-255 values)</span>
<span class="n">si</span><span class="o">.</span><span class="n">save_mask</span><span class="p">(</span><span class="n">out_fn</span> <span class="o">+</span> <span class="s1">&#39;_mask2.jpg&#39;</span><span class="p">,</span> <span class="n">grains</span><span class="p">,</span> <span class="n">image</span><span class="p">,</span> <span class="n">scale</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
</pre></div>
</div>
</section>
<section id="large-image-processing">
<h2>Large image processing<a class="headerlink" href="#large-image-processing" title="Link to this heading">¶</a></h2>
<p>If you want to detect grains in large images, you should use the <code class="docutils literal notranslate"><span class="pre">predict_large_image</span></code> function, which will split the image into patches and run the Unet and SAM segmentations on each patch:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span><span class="w"> </span><span class="nn">PIL</span><span class="w"> </span><span class="kn">import</span> <span class="n">Image</span>
<span class="kn">import</span><span class="w"> </span><span class="nn">segmenteverygrain.interactions</span><span class="w"> </span><span class="k">as</span><span class="w"> </span><span class="nn">si</span>

<span class="n">Image</span><span class="o">.</span><span class="n">MAX_IMAGE_PIXELS</span> <span class="o">=</span> <span class="kc">None</span>  <span class="c1"># needed for very large images</span>

<span class="n">fname</span> <span class="o">=</span> <span class="s2">&quot;./examples/my_large_image.jpg&quot;</span>
<span class="n">image</span> <span class="o">=</span> <span class="n">si</span><span class="o">.</span><span class="n">load_image</span><span class="p">(</span><span class="n">fname</span><span class="p">)</span>

<span class="n">all_grains</span><span class="p">,</span> <span class="n">image_pred</span><span class="p">,</span> <span class="n">all_coords</span> <span class="o">=</span> <span class="n">seg</span><span class="o">.</span><span class="n">predict_large_image</span><span class="p">(</span>
    <span class="n">fname</span><span class="p">,</span> <span class="n">unet</span><span class="p">,</span> <span class="n">sam</span><span class="p">,</span>
    <span class="n">min_area</span><span class="o">=</span><span class="mf">400.0</span><span class="p">,</span>
    <span class="n">patch_size</span><span class="o">=</span><span class="mi">2000</span><span class="p">,</span>
    <span class="n">overlap</span><span class="o">=</span><span class="mi">200</span><span class="p">,</span>
    <span class="n">remove_edge_grains</span><span class="o">=</span><span class="kc">False</span>  <span class="c1"># Keep grains on outer edges of the full image</span>
<span class="p">)</span>
</pre></div>
</div>
<p>Just like before, the <code class="docutils literal notranslate"><span class="pre">all_grains</span></code> list contains shapely polygons of the grains detected in the image. The image containing the grain labels can be generated like this:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">labels</span> <span class="o">=</span> <span class="n">seg</span><span class="o">.</span><span class="n">rasterize_grains</span><span class="p">(</span><span class="n">all_grains</span><span class="p">,</span> <span class="n">image</span><span class="p">)</span>
</pre></div>
</div>
<p>To interactively edit the results, convert to Grain objects and use GrainPlot:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">grains</span> <span class="o">=</span> <span class="n">si</span><span class="o">.</span><span class="n">polygons_to_grains</span><span class="p">(</span><span class="n">all_grains</span><span class="p">,</span> <span class="n">image</span><span class="o">=</span><span class="n">image</span><span class="p">)</span>
<span class="n">predictor</span><span class="o">.</span><span class="n">set_image</span><span class="p">(</span><span class="n">image</span><span class="p">)</span>

<span class="n">plot</span> <span class="o">=</span> <span class="n">si</span><span class="o">.</span><span class="n">GrainPlot</span><span class="p">(</span>
    <span class="n">grains</span><span class="p">,</span>
    <span class="n">image</span><span class="o">=</span><span class="n">image</span><span class="p">,</span>
    <span class="n">predictor</span><span class="o">=</span><span class="n">predictor</span><span class="p">,</span>
    <span class="n">color_palette</span><span class="o">=</span><span class="s1">&#39;tab20b&#39;</span><span class="p">,</span>
<span class="p">)</span>
<span class="n">plot</span><span class="o">.</span><span class="n">activate</span><span class="p">()</span>
</pre></div>
</div>
<p>See the <a class="reference external" href="https://github.com/zsylvester/segmenteverygrain/blob/main/notebooks/Segment_every_grain.ipynb">Segment_every_grain.ipynb</a> notebook for a complete example
of how the models can be loaded and used for segmenting an image and QC-ing the result. The notebook goes through all the steps described above in an interactive format.</p>
</section>
</section>
<section id="grain-extraction-and-clustering">
<h1>Grain extraction and clustering<a class="headerlink" href="#grain-extraction-and-clustering" title="Link to this heading">¶</a></h1>
<p>The <code class="docutils literal notranslate"><span class="pre">grain_utils</span></code> module provides functions for extracting individual grain images and clustering them for classification tasks.</p>
<section id="extracting-individual-grains">
<h2>Extracting individual grains<a class="headerlink" href="#extracting-individual-grains" title="Link to this heading">¶</a></h2>
<p>Extract standardized, square images of individual grains suitable for machine learning:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span><span class="w"> </span><span class="nn">segmenteverygrain</span><span class="w"> </span><span class="kn">import</span> <span class="n">extract_all_grains</span><span class="p">,</span> <span class="n">extract_grain_image</span>

<span class="c1"># Extract all grains as standardized 224x224 images</span>
<span class="n">grain_images</span><span class="p">,</span> <span class="n">grain_masks</span><span class="p">,</span> <span class="n">grain_preds</span> <span class="o">=</span> <span class="n">extract_all_grains</span><span class="p">(</span>
    <span class="n">all_grains</span><span class="p">,</span> <span class="n">image</span><span class="p">,</span> <span class="n">image_pred</span><span class="p">,</span> <span class="n">target_size</span><span class="o">=</span><span class="mi">224</span>
<span class="p">)</span>

<span class="c1"># Or extract a single grain</span>
<span class="n">grain_img</span><span class="p">,</span> <span class="n">grain_mask</span><span class="p">,</span> <span class="n">grain_pred</span><span class="p">,</span> <span class="n">orig_size</span> <span class="o">=</span> <span class="n">extract_grain_image</span><span class="p">(</span>
    <span class="n">all_grains</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="n">image</span><span class="p">,</span> <span class="n">image_pred</span><span class="p">,</span> <span class="n">target_size</span><span class="o">=</span><span class="mi">224</span><span class="p">,</span> <span class="n">pad</span><span class="o">=</span><span class="mi">10</span>
<span class="p">)</span>
</pre></div>
</div>
</section>
<section id="feature-extraction">
<h2>Feature extraction<a class="headerlink" href="#feature-extraction" title="Link to this heading">¶</a></h2>
<p>Extract deep learning features using pre-trained CNNs for clustering or classification:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span><span class="w"> </span><span class="nn">segmenteverygrain</span><span class="w"> </span><span class="kn">import</span> <span class="n">extract_vgg16_features</span><span class="p">,</span> <span class="n">extract_color_features</span>

<span class="c1"># Extract VGG16 features (4096-dimensional)</span>
<span class="n">features</span><span class="p">,</span> <span class="n">model</span> <span class="o">=</span> <span class="n">extract_vgg16_features</span><span class="p">(</span><span class="n">grain_images</span><span class="p">,</span> <span class="n">model_name</span><span class="o">=</span><span class="s1">&#39;VGG16&#39;</span><span class="p">)</span>

<span class="c1"># Or extract color-based features</span>
<span class="n">color_features</span> <span class="o">=</span> <span class="n">extract_color_features</span><span class="p">(</span><span class="n">grain_images</span><span class="p">,</span> <span class="n">color_space</span><span class="o">=</span><span class="s1">&#39;hsv&#39;</span><span class="p">)</span>
</pre></div>
</div>
</section>
<section id="clustering-grains">
<h2>Clustering grains<a class="headerlink" href="#clustering-grains" title="Link to this heading">¶</a></h2>
<p>Cluster grains based on their features:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span><span class="w"> </span><span class="nn">segmenteverygrain</span><span class="w"> </span><span class="kn">import</span> <span class="n">cluster_grains</span><span class="p">,</span> <span class="n">create_clustered_grain_montage</span>

<span class="c1"># Cluster using K-means with PCA dimensionality reduction</span>
<span class="n">labels</span><span class="p">,</span> <span class="n">reduced_features</span><span class="p">,</span> <span class="n">pca</span><span class="p">,</span> <span class="n">clusterer</span> <span class="o">=</span> <span class="n">cluster_grains</span><span class="p">(</span>
    <span class="n">features</span><span class="p">,</span> <span class="n">n_clusters</span><span class="o">=</span><span class="mi">10</span><span class="p">,</span> <span class="n">n_components</span><span class="o">=</span><span class="mi">25</span>
<span class="p">)</span>

<span class="c1"># Create a visual montage of clustered grains</span>
<span class="n">montage</span><span class="p">,</span> <span class="n">cluster_info</span> <span class="o">=</span> <span class="n">create_clustered_grain_montage</span><span class="p">(</span>
    <span class="n">labels</span><span class="p">,</span> <span class="n">grain_images</span><span class="p">,</span> <span class="n">grid_cols</span><span class="o">=</span><span class="mi">20</span><span class="p">,</span> <span class="n">draw_boundaries</span><span class="o">=</span><span class="kc">True</span>
<span class="p">)</span>
</pre></div>
</div>
</section>
<section id="interactive-grain-selection-and-labeling">
<h2>Interactive grain selection and labeling<a class="headerlink" href="#interactive-grain-selection-and-labeling" title="Link to this heading">¶</a></h2>
<p>Use the <code class="docutils literal notranslate"><span class="pre">ClusterMontageSelector</span></code> for quality control (removing bad grains or clusters):</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span><span class="w"> </span><span class="nn">segmenteverygrain</span><span class="w"> </span><span class="kn">import</span> <span class="n">ClusterMontageSelector</span>

<span class="n">selector</span> <span class="o">=</span> <span class="n">ClusterMontageSelector</span><span class="p">(</span><span class="n">labels</span><span class="p">,</span> <span class="n">grain_images</span><span class="p">,</span> <span class="n">all_grains</span><span class="p">)</span>
<span class="n">selector</span><span class="o">.</span><span class="n">activate</span><span class="p">()</span>  <span class="c1"># Interactive mode</span>

<span class="c1"># After selection, get filtered results</span>
<span class="n">filtered_grains</span> <span class="o">=</span> <span class="n">selector</span><span class="o">.</span><span class="n">get_filtered_grains</span><span class="p">()</span>
</pre></div>
</div>
<p>Use the <code class="docutils literal notranslate"><span class="pre">ClusterMontageLabeler</span></code> for labeling grains with custom categories:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span><span class="w"> </span><span class="nn">segmenteverygrain</span><span class="w"> </span><span class="kn">import</span> <span class="n">ClusterMontageLabeler</span>

<span class="n">labeler</span> <span class="o">=</span> <span class="n">ClusterMontageLabeler</span><span class="p">(</span>
    <span class="n">labels</span><span class="p">,</span> <span class="n">grain_images</span><span class="p">,</span> <span class="n">all_grains</span><span class="p">,</span>
    <span class="n">label_names</span><span class="o">=</span><span class="p">[</span><span class="s1">&#39;quartz&#39;</span><span class="p">,</span> <span class="s1">&#39;feldspar&#39;</span><span class="p">,</span> <span class="s1">&#39;lithic&#39;</span><span class="p">,</span> <span class="s1">&#39;other&#39;</span><span class="p">]</span>
<span class="p">)</span>
<span class="n">labeler</span><span class="o">.</span><span class="n">activate</span><span class="p">()</span>  <span class="c1"># Interactive mode</span>

<span class="c1"># Export labels</span>
<span class="n">labeler</span><span class="o">.</span><span class="n">export_labels</span><span class="p">(</span><span class="s1">&#39;grain_labels.csv&#39;</span><span class="p">)</span>
<span class="n">labeler</span><span class="o">.</span><span class="n">save_labeled_images</span><span class="p">(</span><span class="s1">&#39;labeled_grains/&#39;</span><span class="p">)</span>
</pre></div>
</div>
</section>
<section id="visualizing-classified-grains">
<h2>Visualizing classified grains<a class="headerlink" href="#visualizing-classified-grains" title="Link to this heading">¶</a></h2>
<p>Plot grains colored by their classification:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span><span class="w"> </span><span class="nn">segmenteverygrain</span><span class="w"> </span><span class="kn">import</span> <span class="n">plot_classified_grains</span>

<span class="n">fig</span><span class="p">,</span> <span class="n">ax</span> <span class="o">=</span> <span class="n">plot_classified_grains</span><span class="p">(</span>
    <span class="n">image</span><span class="p">,</span> <span class="n">all_grains</span><span class="p">,</span> <span class="n">classifications</span><span class="p">,</span>
    <span class="n">class_colors</span><span class="o">=</span><span class="p">{</span><span class="s1">&#39;quartz&#39;</span><span class="p">:</span> <span class="s1">&#39;blue&#39;</span><span class="p">,</span> <span class="s1">&#39;feldspar&#39;</span><span class="p">:</span> <span class="s1">&#39;red&#39;</span><span class="p">,</span> <span class="s1">&#39;other&#39;</span><span class="p">:</span> <span class="s1">&#39;green&#39;</span><span class="p">}</span>
<span class="p">)</span>
</pre></div>
</div>
</section>
<section id="hardware-requirements">
<h2>Hardware requirements<a class="headerlink" href="#hardware-requirements" title="Link to this heading">¶</a></h2>
<p>For training a new U-Net model or fine tuning the existing one, GPU access is necessary. The easiest way of getting access to a powerful GPU is Google Colab. In inference mode, a moderately powerful computer with at least 16 GB of memory should be enough. That said, larger CPU speeds and more memory will significantly reduce inference time.</p>
</section>
</section>
<section id="finetuning-the-u-net-model">
<h1>Finetuning the U-Net model<a class="headerlink" href="#finetuning-the-u-net-model" title="Link to this heading">¶</a></h1>
<p>The last section of the <a class="reference external" href="https://github.com/zsylvester/segmenteverygrain/blob/main/notebooks/Segment_every_grain.ipynb">Segment_every_grain.ipynb</a> notebook shows how to finetune the U-Net model. The first step is to create patches (usually 256x256 pixels in size) from the images and the corresponding masks that you want to use for training.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">image_dir</span><span class="p">,</span> <span class="n">mask_dir</span> <span class="o">=</span> <span class="n">seg</span><span class="o">.</span><span class="n">patchify_training_data</span><span class="p">(</span><span class="n">input_dir</span><span class="p">,</span> <span class="n">patch_dir</span><span class="p">)</span>
</pre></div>
</div>
<p>The <code class="docutils literal notranslate"><span class="pre">input_dir</span></code> should contain the images and masks that you want to use for training. These files should have ‘image’ and ‘mask’ in their filenames, for example, ‘sample1_image.png’ and ‘sample1_mask.png’. An example image can be found <a class="reference external" href="https://github.com/zsylvester/segmenteverygrain/blob/main/torrey_pines_beach_image.jpeg">here</a>; and the corresponding mask is <a class="reference external" href="https://github.com/zsylvester/segmenteverygrain/blob/main/torrey_pines_beach_mask.png">here</a>.</p>
<p>The mask is an 8-bit image and should contain only three numbers: 0, 1, and 2. 0 is the background, 1 is the grain, and 2 is the grain boundary. Usually the mask is generated using the <code class="docutils literal notranslate"><span class="pre">segmenteverygrain</span></code> workflow, that is, by running the U-Net segmentation first, the SAM segmentation second, and then cleaning up the result. That said, when the U-Net ouputs are of low quality, it might be a good idea to generate the masks directly with SAM. Once you have a good mask, you can save it using <code class="docutils literal notranslate"><span class="pre">cv2.imwrite</span></code> (see also the example notebook):</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">cv2</span><span class="o">.</span><span class="n">imwrite</span><span class="p">(</span><span class="s1">&#39;sample1_mask.png&#39;</span><span class="p">,</span> <span class="n">mask</span><span class="p">)</span>
</pre></div>
</div>
<p>The <code class="docutils literal notranslate"><span class="pre">patch_dir</span></code> is the directory where the patches will be saved. A folder named ‘Patches’ will be created in this directory, and the patches will be saved in subfolders named ‘images’ and ‘labels’.</p>
<p>Next, training, validation, and test datasets are created from the patches:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">train_dataset</span><span class="p">,</span> <span class="n">val_dataset</span><span class="p">,</span> <span class="n">test_dataset</span> <span class="o">=</span> <span class="n">seg</span><span class="o">.</span><span class="n">create_train_val_test_data</span><span class="p">(</span><span class="n">image_dir</span><span class="p">,</span> <span class="n">mask_dir</span><span class="p">,</span> <span class="n">augmentation</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
</pre></div>
</div>
<p>Now we are ready to load the existing model weights and to train the model:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">model</span> <span class="o">=</span> <span class="n">seg</span><span class="o">.</span><span class="n">create_and_train_model</span><span class="p">(</span><span class="n">train_dataset</span><span class="p">,</span> <span class="n">val_dataset</span><span class="p">,</span> <span class="n">test_dataset</span><span class="p">,</span> <span class="n">model_file</span><span class="o">=</span><span class="s1">&#39;seg_model.keras&#39;</span><span class="p">,</span> <span class="n">epochs</span><span class="o">=</span><span class="mi">100</span><span class="p">)</span>
</pre></div>
</div>
<p>If you are happy with the finetuned model, you will want to save it:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">model</span><span class="o">.</span><span class="n">save</span><span class="p">(</span><span class="s1">&#39;seg_model_finetuned.keras&#39;</span><span class="p">)</span>
</pre></div>
</div>
<p>If you want to use this new model to make predictions, you will need to load it with the custom loss function:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">model</span> <span class="o">=</span> <span class="n">load_model</span><span class="p">(</span><span class="s2">&quot;seg_model_finetuned.keras&quot;</span><span class="p">,</span> <span class="n">custom_objects</span><span class="o">=</span><span class="p">{</span><span class="s1">&#39;weighted_crossentropy&#39;</span><span class="p">:</span> <span class="n">seg</span><span class="o">.</span><span class="n">weighted_crossentropy</span><span class="p">})</span>
</pre></div>
</div>
</section>
<section id="training-the-u-net-model-from-scratch">
<h1>Training the U-Net model from scratch<a class="headerlink" href="#training-the-u-net-model-from-scratch" title="Link to this heading">¶</a></h1>
<p>If you want to train a U-Net model from scratch, you can use the <a class="reference external" href="https://github.com/zsylvester/segmenteverygrain/blob/main/notebooks/Train_Unet_model.ipynb">Train_Unet_model.ipynb</a> notebook, which mostly consists of the code snippets below.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span><span class="w"> </span><span class="nn">segmenteverygrain</span><span class="w"> </span><span class="k">as</span><span class="w"> </span><span class="nn">seg</span>
<span class="n">model</span> <span class="o">=</span> <span class="n">seg</span><span class="o">.</span><span class="n">Unet</span><span class="p">()</span> <span class="c1"># create model</span>
<span class="n">model</span><span class="o">.</span><span class="n">compile</span><span class="p">(</span><span class="n">optimizer</span><span class="o">=</span><span class="n">Adam</span><span class="p">(),</span> <span class="n">loss</span><span class="o">=</span><span class="n">seg</span><span class="o">.</span><span class="n">weighted_crossentropy</span><span class="p">,</span> <span class="n">metrics</span><span class="o">=</span><span class="p">[</span><span class="s2">&quot;accuracy&quot;</span><span class="p">])</span>
</pre></div>
</div>
<p>If you place the training images and masks in the ‘images’ directory (the filenames are supposed to terminate with ‘_image.png’ and ‘_mask.png’), you can create the training dataset like this:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">input_dir</span> <span class="o">=</span> <span class="s2">&quot;../images/&quot;</span>
<span class="n">patch_dir</span> <span class="o">=</span> <span class="s2">&quot;../patches/&quot;</span>
<span class="n">image_dir</span><span class="p">,</span> <span class="n">mask_dir</span> <span class="o">=</span> <span class="n">seg</span><span class="o">.</span><span class="n">patchify_training_data</span><span class="p">(</span><span class="n">input_dir</span><span class="p">,</span> <span class="n">patch_dir</span><span class="p">)</span>
<span class="n">image_dir</span> <span class="o">=</span> <span class="s1">&#39;../patches/Patches/images&#39;</span>
<span class="n">mask_dir</span> <span class="o">=</span> <span class="s1">&#39;../patches/Patches/labels&#39;</span>
<span class="n">train_dataset</span><span class="p">,</span> <span class="n">val_dataset</span><span class="p">,</span> <span class="n">test_dataset</span> <span class="o">=</span> <span class="n">seg</span><span class="o">.</span><span class="n">create_train_val_test_data</span><span class="p">(</span><span class="n">image_dir</span><span class="p">,</span> <span class="n">mask_dir</span><span class="p">,</span> <span class="n">augmentation</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
</pre></div>
</div>
<p>Then you can train and test the model:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">model</span> <span class="o">=</span> <span class="n">seg</span><span class="o">.</span><span class="n">create_and_train_model</span><span class="p">(</span><span class="n">train_dataset</span><span class="p">,</span> <span class="n">val_dataset</span><span class="p">,</span> <span class="n">test_dataset</span><span class="p">,</span> <span class="n">epochs</span><span class="o">=</span><span class="mi">200</span><span class="p">)</span>
</pre></div>
</div>
<p>The model can be saved using the Keras save method:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">model</span><span class="o">.</span><span class="n">save</span><span class="p">(</span><span class="s1">&#39;seg_model.keras&#39;</span><span class="p">)</span>
</pre></div>
</div>
<p>The U-Net model in the GitHub repository was trained using 66 images and the corresponding masks of a variety of grains, split into 44,533 patches of 256x256 pixels. 48 of these image-mask pairs are available at this Zenodo repository: <a class="reference external" href="https://zenodo.org/records/15786086">https://zenodo.org/records/15786086</a>. The model was trained for 200 epochs with a batch size of 32, using the Adam optimizer and a weighted cross-entropy loss function. The training accuracy was 0.937, the validation accuracy was 0.922, and the testing accuracy was 0.922 at the end of training. The model is available in the repository as ‘seg_model.keras’.</p>
</section>


          </div>
          
        </div>
      </div>
      <div class="sphinxsidebar" role="navigation" aria-label="Main">
        <div class="sphinxsidebarwrapper">
<h1 class="logo"><a href="index.html">segmenteverygrain</a></h1>









<search id="searchbox" style="display: none" role="search">
    <div class="searchformwrapper">
    <form class="search" action="search.html" method="get">
      <input type="text" name="q" aria-labelledby="searchlabel" autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false" placeholder="Search"/>
      <input type="submit" value="Go" />
    </form>
    </div>
</search>
<script>document.getElementById('searchbox').style.display = "block"</script><h3>Navigation</h3>
<ul class="current">
<li class="toctree-l1 current"><a class="current reference internal" href="#">Getting started</a><ul>
<li class="toctree-l2"><a class="reference internal" href="#loading-the-models">Loading the models</a></li>
<li class="toctree-l2"><a class="reference internal" href="#running-the-segmentation">Running the segmentation</a></li>
<li class="toctree-l2"><a class="reference internal" href="#quality-control-of-u-net-prediction">Quality control of U-Net prediction</a></li>
<li class="toctree-l2"><a class="reference internal" href="#sam-segmentation">SAM segmentation</a></li>
<li class="toctree-l2"><a class="reference internal" href="#interactive-editing-of-results">Interactive editing of results</a></li>
<li class="toctree-l2"><a class="reference internal" href="#scale-bar-and-unit-conversion">Scale bar and unit conversion</a></li>
<li class="toctree-l2"><a class="reference internal" href="#grain-size-analysis">Grain size analysis</a></li>
<li class="toctree-l2"><a class="reference internal" href="#saving-results">Saving results</a></li>
<li class="toctree-l2"><a class="reference internal" href="#large-image-processing">Large image processing</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="#grain-extraction-and-clustering">Grain extraction and clustering</a><ul>
<li class="toctree-l2"><a class="reference internal" href="#extracting-individual-grains">Extracting individual grains</a></li>
<li class="toctree-l2"><a class="reference internal" href="#feature-extraction">Feature extraction</a></li>
<li class="toctree-l2"><a class="reference internal" href="#clustering-grains">Clustering grains</a></li>
<li class="toctree-l2"><a class="reference internal" href="#interactive-grain-selection-and-labeling">Interactive grain selection and labeling</a></li>
<li class="toctree-l2"><a class="reference internal" href="#visualizing-classified-grains">Visualizing classified grains</a></li>
<li class="toctree-l2"><a class="reference internal" href="#hardware-requirements">Hardware requirements</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="#finetuning-the-u-net-model">Finetuning the U-Net model</a></li>
<li class="toctree-l1"><a class="reference internal" href="#training-the-u-net-model-from-scratch">Training the U-Net model from scratch</a></li>
<li class="toctree-l1"><a class="reference internal" href="api_reference.html">API Reference</a></li>
</ul>

<div class="relations">
<h3>Related Topics</h3>
<ul>
  <li><a href="index.html">Documentation overview</a><ul>
      <li>Previous: <a href="index.html" title="previous chapter">segmenteverygrain</a></li>
      <li>Next: <a href="api_reference.html" title="next chapter">API Reference</a></li>
  </ul></li>
</ul>
</div>








        </div>
      </div>
      <div class="clearer"></div>
    </div>
    <div class="footer">
      &#169;2024, Zoltan Sylvester.
      
      |
      Powered by <a href="https://www.sphinx-doc.org/">Sphinx 8.1.3</a>
      &amp; <a href="https://alabaster.readthedocs.io">Alabaster 1.0.0</a>
      
      |
      <a href="_sources/getting_started.rst.txt"
          rel="nofollow">Page source</a>
    </div>

    

    
  </body>
</html>