<!DOCTYPE html>

<html lang="en" data-content_root="./">
  <head>
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" /><meta name="viewport" content="width=device-width, initial-scale=1" />

    <title>Getting started &#8212; segmenteverygrain 0.2.1 documentation</title>
    <link rel="stylesheet" type="text/css" href="_static/pygments.css?v=5ecbeea2" />
    <link rel="stylesheet" type="text/css" href="_static/alabaster.css?v=12dfc556" />
    <script src="_static/documentation_options.js?v=37f418d5"></script>
    <script src="_static/doctools.js?v=9a2dae69"></script>
    <script src="_static/sphinx_highlight.js?v=dc90522c"></script>
    <link rel="index" title="Index" href="genindex.html" />
    <link rel="search" title="Search" href="search.html" />
    <link rel="next" title="API Reference" href="api_reference.html" />
    <link rel="prev" title="segmenteverygrain" href="index.html" />
   
  <link rel="stylesheet" href="_static/custom.css" type="text/css" />
  

  
  

  </head><body>
  

    <div class="document">
      <div class="documentwrapper">
        <div class="bodywrapper">
          

          <div class="body" role="main">
            
  <section id="getting-started">
<h1>Getting started<a class="headerlink" href="#getting-started" title="Link to this heading">¶</a></h1>
<div class="toctree-wrapper compound">
</div>
<p>The best way to use the <cite>segmenteverygrain</cite> package is to run the <a class="reference external" href="https://github.com/zsylvester/segmenteverygrain/blob/main/segmenteverygrain/Segment_every_grain.ipynb">Segment_every_grain.ipynb</a> notebook.</p>
<p>The notebook goes through the steps of loading the models, running the segmentation, interactively updating the result, and saving the grain data and the mask. The text below summarizes the steps that you need to take to run the segmentation.</p>
<section id="loading-the-models">
<h2>Loading the models<a class="headerlink" href="#loading-the-models" title="Link to this heading">¶</a></h2>
<p>To load the U-Net model, you can use the ‘load_model’ function from Keras. The U-Net model is saved in the ‘seg_model.keras’ file.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span><span class="w"> </span><span class="nn">segmenteverygrain</span><span class="w"> </span><span class="k">as</span><span class="w"> </span><span class="nn">seg</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">keras.saving</span><span class="w"> </span><span class="kn">import</span> <span class="n">load_model</span>
<span class="n">model</span> <span class="o">=</span> <span class="n">load_model</span><span class="p">(</span><span class="s2">&quot;seg_model.keras&quot;</span><span class="p">,</span> <span class="n">custom_objects</span><span class="o">=</span><span class="p">{</span><span class="s1">&#39;weighted_crossentropy&#39;</span><span class="p">:</span> <span class="n">seg</span><span class="o">.</span><span class="n">weighted_crossentropy</span><span class="p">})</span>
</pre></div>
</div>
<p>This assumes that you are using Keras 3 and ‘seg_model.keras’ was saved using Keras 3. Older models created with a <code class="docutils literal notranslate"><span class="pre">segmenteverygrain</span></code> version that was based on Keras 2 do not work with with the latest version of the package.</p>
<p>The Segment Anything model can be downloaded from this <a class="reference external" href="https://dl.fbaipublicfiles.com/segment_anything/sam_vit_h_4b8939.pth">link</a>. You can also download it programmatically:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span><span class="w"> </span><span class="nn">os</span>
<span class="kn">import</span><span class="w"> </span><span class="nn">urllib.request</span>

<span class="k">if</span> <span class="ow">not</span> <span class="n">os</span><span class="o">.</span><span class="n">path</span><span class="o">.</span><span class="n">exists</span><span class="p">(</span><span class="s2">&quot;./models/sam_vit_h_4b8939.pth&quot;</span><span class="p">):</span>
    <span class="n">url</span> <span class="o">=</span> <span class="s2">&quot;https://dl.fbaipublicfiles.com/segment_anything/sam_vit_h_4b8939.pth&quot;</span>
    <span class="n">urllib</span><span class="o">.</span><span class="n">request</span><span class="o">.</span><span class="n">urlretrieve</span><span class="p">(</span><span class="n">url</span><span class="p">,</span> <span class="s2">&quot;./models/sam_vit_h_4b8939.pth&quot;</span><span class="p">)</span>
</pre></div>
</div>
</section>
<section id="running-the-segmentation">
<h2>Running the segmentation<a class="headerlink" href="#running-the-segmentation" title="Link to this heading">¶</a></h2>
<p>To run the U-Net segmentation on an image and label the grains in the U-Net output:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span><span class="w"> </span><span class="nn">numpy</span><span class="w"> </span><span class="k">as</span><span class="w"> </span><span class="nn">np</span>
<span class="kn">import</span><span class="w"> </span><span class="nn">matplotlib.pyplot</span><span class="w"> </span><span class="k">as</span><span class="w"> </span><span class="nn">plt</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">keras.utils</span><span class="w"> </span><span class="kn">import</span> <span class="n">load_img</span>

<span class="c1"># Load your image</span>
<span class="n">fname</span> <span class="o">=</span> <span class="s2">&quot;path/to/your/image.jpg&quot;</span>
<span class="n">image</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">(</span><span class="n">load_img</span><span class="p">(</span><span class="n">fname</span><span class="p">))</span>

<span class="c1"># Run U-Net prediction</span>
<span class="n">image_pred</span> <span class="o">=</span> <span class="n">seg</span><span class="o">.</span><span class="n">predict_image</span><span class="p">(</span><span class="n">image</span><span class="p">,</span> <span class="n">model</span><span class="p">,</span> <span class="n">I</span><span class="o">=</span><span class="mi">256</span><span class="p">)</span>
<span class="n">labels</span><span class="p">,</span> <span class="n">coords</span> <span class="o">=</span> <span class="n">seg</span><span class="o">.</span><span class="n">label_grains</span><span class="p">(</span><span class="n">image</span><span class="p">,</span> <span class="n">image_pred</span><span class="p">,</span> <span class="n">dbs_max_dist</span><span class="o">=</span><span class="mf">20.0</span><span class="p">)</span>
</pre></div>
</div>
<p>The input image should not be much larger than ~2000x3000 pixels, in part to avoid long running times; it is supposed to be a numpy array with 3 channels (RGB).
Grains should be well defined in the image and not too small (e.g., only a few pixels in size).</p>
</section>
<section id="quality-control-of-u-net-prediction">
<h2>Quality control of U-Net prediction<a class="headerlink" href="#quality-control-of-u-net-prediction" title="Link to this heading">¶</a></h2>
<p>The U-Net prediction should be QC-d before running the SAM segmentation:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">plt</span><span class="o">.</span><span class="n">figure</span><span class="p">(</span><span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">15</span><span class="p">,</span><span class="mi">10</span><span class="p">))</span>
<span class="n">plt</span><span class="o">.</span><span class="n">imshow</span><span class="p">(</span><span class="n">image_pred</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">scatter</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">(</span><span class="n">coords</span><span class="p">)[:,</span><span class="mi">0</span><span class="p">],</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">(</span><span class="n">coords</span><span class="p">)[:,</span><span class="mi">1</span><span class="p">],</span> <span class="n">c</span><span class="o">=</span><span class="s1">&#39;k&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">xticks</span><span class="p">([])</span>
<span class="n">plt</span><span class="o">.</span><span class="n">yticks</span><span class="p">([])</span>
</pre></div>
</div>
<p>The black dots in the figure represent the SAM prompts that will be used for grain segmentation. If the U-Net segmentation is of low quality, the base model can be (and should be) finetuned using the steps outlined <a class="reference internal" href="#finetuning-the-u-net-model"><span class="std std-ref">below</span></a>.</p>
</section>
<section id="sam-segmentation">
<h2>SAM segmentation<a class="headerlink" href="#sam-segmentation" title="Link to this heading">¶</a></h2>
<p>Here is an example showing how to run the SAM segmentation on an image, using the outputs from the U-Net model:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span><span class="w"> </span><span class="nn">segment_anything</span><span class="w"> </span><span class="kn">import</span> <span class="n">sam_model_registry</span>
<span class="n">sam</span> <span class="o">=</span> <span class="n">sam_model_registry</span><span class="p">[</span><span class="s2">&quot;default&quot;</span><span class="p">](</span><span class="n">checkpoint</span><span class="o">=</span><span class="s2">&quot;sam_vit_h_4b8939.pth&quot;</span><span class="p">)</span> <span class="c1"># load the SAM model</span>
<span class="n">all_grains</span><span class="p">,</span> <span class="n">labels</span><span class="p">,</span> <span class="n">mask_all</span><span class="p">,</span> <span class="n">grain_data</span><span class="p">,</span> <span class="n">fig</span><span class="p">,</span> <span class="n">ax</span> <span class="o">=</span> <span class="n">seg</span><span class="o">.</span><span class="n">sam_segmentation</span><span class="p">(</span><span class="n">sam</span><span class="p">,</span> <span class="n">image</span><span class="p">,</span> <span class="n">image_pred</span><span class="p">,</span> <span class="n">coords</span><span class="p">,</span> <span class="n">labels</span><span class="p">,</span> <span class="n">min_area</span><span class="o">=</span><span class="mf">400.0</span><span class="p">,</span> <span class="n">plot_image</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">remove_edge_grains</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> <span class="n">remove_large_objects</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>
</pre></div>
</div>
<p>The <code class="docutils literal notranslate"><span class="pre">all_grains</span></code> list contains shapely polygons of the grains detected in the image. <code class="docutils literal notranslate"><span class="pre">labels</span></code> is an image that contains the labels of the grains.
<code class="docutils literal notranslate"><span class="pre">grain_data</span></code> is a pandas dataframe with a number of grain parameters.</p>
</section>
<section id="interactive-editing-of-results">
<h2>Interactive editing of results<a class="headerlink" href="#interactive-editing-of-results" title="Link to this heading">¶</a></h2>
<p>After the initial segmentation, you can interactively edit the results to delete unwanted grains or merge grains that should be combined:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">grain_inds</span> <span class="o">=</span> <span class="p">[]</span>
<span class="n">cid1</span> <span class="o">=</span> <span class="n">fig</span><span class="o">.</span><span class="n">canvas</span><span class="o">.</span><span class="n">mpl_connect</span><span class="p">(</span>
    <span class="s2">&quot;button_press_event&quot;</span><span class="p">,</span>
    <span class="k">lambda</span> <span class="n">event</span><span class="p">:</span> <span class="n">seg</span><span class="o">.</span><span class="n">onclick2</span><span class="p">(</span><span class="n">event</span><span class="p">,</span> <span class="n">all_grains</span><span class="p">,</span> <span class="n">grain_inds</span><span class="p">,</span> <span class="n">ax</span><span class="o">=</span><span class="n">ax</span><span class="p">),</span>
<span class="p">)</span>
<span class="n">cid2</span> <span class="o">=</span> <span class="n">fig</span><span class="o">.</span><span class="n">canvas</span><span class="o">.</span><span class="n">mpl_connect</span><span class="p">(</span>
    <span class="s2">&quot;key_press_event&quot;</span><span class="p">,</span>
    <span class="k">lambda</span> <span class="n">event</span><span class="p">:</span> <span class="n">seg</span><span class="o">.</span><span class="n">onpress2</span><span class="p">(</span><span class="n">event</span><span class="p">,</span> <span class="n">all_grains</span><span class="p">,</span> <span class="n">grain_inds</span><span class="p">,</span> <span class="n">fig</span><span class="o">=</span><span class="n">fig</span><span class="p">,</span> <span class="n">ax</span><span class="o">=</span><span class="n">ax</span><span class="p">),</span>
<span class="p">)</span>
</pre></div>
</div>
<p>Interactive controls:</p>
<ul class="simple">
<li><p>Click on a grain that you want to remove and press the ‘x’ key</p></li>
<li><p>Click on two grains that you want to merge and press the ‘m’ key (they have to be the last two grains you clicked on)</p></li>
<li><p>Press the ‘g’ key to hide the grain masks; press the ‘g’ key again to show the grain masks</p></li>
</ul>
<p>After editing, disconnect the event handlers and update the grain data:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">fig</span><span class="o">.</span><span class="n">canvas</span><span class="o">.</span><span class="n">mpl_disconnect</span><span class="p">(</span><span class="n">cid1</span><span class="p">)</span>
<span class="n">fig</span><span class="o">.</span><span class="n">canvas</span><span class="o">.</span><span class="n">mpl_disconnect</span><span class="p">(</span><span class="n">cid2</span><span class="p">)</span>
<span class="n">all_grains</span><span class="p">,</span> <span class="n">labels</span><span class="p">,</span> <span class="n">mask_all</span> <span class="o">=</span> <span class="n">seg</span><span class="o">.</span><span class="n">get_grains_from_patches</span><span class="p">(</span><span class="n">ax</span><span class="p">,</span> <span class="n">image</span><span class="p">)</span>
</pre></div>
</div>
</section>
<section id="adding-new-grains">
<h2>Adding new grains<a class="headerlink" href="#adding-new-grains" title="Link to this heading">¶</a></h2>
<p>You can also add new grains using the Segment Anything Model:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span><span class="w"> </span><span class="nn">segment_anything</span><span class="w"> </span><span class="kn">import</span> <span class="n">SamPredictor</span>

<span class="n">predictor</span> <span class="o">=</span> <span class="n">SamPredictor</span><span class="p">(</span><span class="n">sam</span><span class="p">)</span>
<span class="n">predictor</span><span class="o">.</span><span class="n">set_image</span><span class="p">(</span><span class="n">image</span><span class="p">)</span>  <span class="c1"># this can take a while</span>
<span class="n">coords</span> <span class="o">=</span> <span class="p">[]</span>
<span class="n">cid3</span> <span class="o">=</span> <span class="n">fig</span><span class="o">.</span><span class="n">canvas</span><span class="o">.</span><span class="n">mpl_connect</span><span class="p">(</span>
    <span class="s2">&quot;button_press_event&quot;</span><span class="p">,</span> <span class="k">lambda</span> <span class="n">event</span><span class="p">:</span> <span class="n">seg</span><span class="o">.</span><span class="n">onclick</span><span class="p">(</span><span class="n">event</span><span class="p">,</span> <span class="n">ax</span><span class="p">,</span> <span class="n">coords</span><span class="p">,</span> <span class="n">image</span><span class="p">,</span> <span class="n">predictor</span><span class="p">)</span>
<span class="p">)</span>
<span class="n">cid4</span> <span class="o">=</span> <span class="n">fig</span><span class="o">.</span><span class="n">canvas</span><span class="o">.</span><span class="n">mpl_connect</span><span class="p">(</span>
    <span class="s2">&quot;key_press_event&quot;</span><span class="p">,</span> <span class="k">lambda</span> <span class="n">event</span><span class="p">:</span> <span class="n">seg</span><span class="o">.</span><span class="n">onpress</span><span class="p">(</span><span class="n">event</span><span class="p">,</span> <span class="n">ax</span><span class="p">,</span> <span class="n">fig</span><span class="p">)</span>
<span class="p">)</span>
</pre></div>
</div>
<p>Interactive controls for adding grains:</p>
<ul class="simple">
<li><p>Click on an unsegmented grain that you want to add</p></li>
<li><p>Press the ‘x’ key to delete the last grain you added</p></li>
<li><p>Press the ‘m’ key to merge the last two grains that you added</p></li>
<li><p>Right click outside the grain (but inside the most recent mask) to restrict the grain to a smaller mask</p></li>
</ul>
</section>
<section id="grain-size-analysis">
<h2>Grain size analysis<a class="headerlink" href="#grain-size-analysis" title="Link to this heading">¶</a></h2>
<p>To perform grain size analysis, you first need to establish the scale of your image by clicking on a scale bar:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">cid5</span> <span class="o">=</span> <span class="n">fig</span><span class="o">.</span><span class="n">canvas</span><span class="o">.</span><span class="n">mpl_connect</span><span class="p">(</span>
    <span class="s2">&quot;button_press_event&quot;</span><span class="p">,</span> <span class="k">lambda</span> <span class="n">event</span><span class="p">:</span> <span class="n">seg</span><span class="o">.</span><span class="n">click_for_scale</span><span class="p">(</span><span class="n">event</span><span class="p">,</span> <span class="n">ax</span><span class="p">)</span>
<span class="p">)</span>
</pre></div>
</div>
<p>Click on one end of the scale bar with the left mouse button and on the other end with the right mouse button. Then calculate the scale:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">n_of_units</span> <span class="o">=</span> <span class="mf">10.0</span>  <span class="c1"># length of scale bar in real units (e.g., centimeters)</span>
<span class="n">units_per_pixel</span> <span class="o">=</span> <span class="n">n_of_units</span> <span class="o">/</span> <span class="n">scale_bar_length_pixels</span>  <span class="c1"># from the output above</span>
</pre></div>
</div>
<p>Calculate grain properties and create a dataframe:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span><span class="w"> </span><span class="nn">skimage.measure</span><span class="w"> </span><span class="kn">import</span> <span class="n">regionprops_table</span>
<span class="kn">import</span><span class="w"> </span><span class="nn">pandas</span><span class="w"> </span><span class="k">as</span><span class="w"> </span><span class="nn">pd</span>

<span class="n">props</span> <span class="o">=</span> <span class="n">regionprops_table</span><span class="p">(</span>
    <span class="n">labels</span><span class="o">.</span><span class="n">astype</span><span class="p">(</span><span class="s2">&quot;int&quot;</span><span class="p">),</span>
    <span class="n">intensity_image</span><span class="o">=</span><span class="n">image</span><span class="p">,</span>
    <span class="n">properties</span><span class="o">=</span><span class="p">(</span>
        <span class="s2">&quot;label&quot;</span><span class="p">,</span> <span class="s2">&quot;area&quot;</span><span class="p">,</span> <span class="s2">&quot;centroid&quot;</span><span class="p">,</span> <span class="s2">&quot;major_axis_length&quot;</span><span class="p">,</span>
        <span class="s2">&quot;minor_axis_length&quot;</span><span class="p">,</span> <span class="s2">&quot;orientation&quot;</span><span class="p">,</span> <span class="s2">&quot;perimeter&quot;</span><span class="p">,</span>
        <span class="s2">&quot;max_intensity&quot;</span><span class="p">,</span> <span class="s2">&quot;mean_intensity&quot;</span><span class="p">,</span> <span class="s2">&quot;min_intensity&quot;</span><span class="p">,</span>
    <span class="p">),</span>
<span class="p">)</span>
<span class="n">grain_data</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">DataFrame</span><span class="p">(</span><span class="n">props</span><span class="p">)</span>

<span class="c1"># Convert pixel measurements to real units</span>
<span class="n">grain_data</span><span class="p">[</span><span class="s2">&quot;major_axis_length&quot;</span><span class="p">]</span> <span class="o">=</span> <span class="n">grain_data</span><span class="p">[</span><span class="s2">&quot;major_axis_length&quot;</span><span class="p">]</span><span class="o">.</span><span class="n">values</span> <span class="o">*</span> <span class="n">units_per_pixel</span>
<span class="n">grain_data</span><span class="p">[</span><span class="s2">&quot;minor_axis_length&quot;</span><span class="p">]</span> <span class="o">=</span> <span class="n">grain_data</span><span class="p">[</span><span class="s2">&quot;minor_axis_length&quot;</span><span class="p">]</span><span class="o">.</span><span class="n">values</span> <span class="o">*</span> <span class="n">units_per_pixel</span>
<span class="n">grain_data</span><span class="p">[</span><span class="s2">&quot;perimeter&quot;</span><span class="p">]</span> <span class="o">=</span> <span class="n">grain_data</span><span class="p">[</span><span class="s2">&quot;perimeter&quot;</span><span class="p">]</span><span class="o">.</span><span class="n">values</span> <span class="o">*</span> <span class="n">units_per_pixel</span>
<span class="n">grain_data</span><span class="p">[</span><span class="s2">&quot;area&quot;</span><span class="p">]</span> <span class="o">=</span> <span class="n">grain_data</span><span class="p">[</span><span class="s2">&quot;area&quot;</span><span class="p">]</span><span class="o">.</span><span class="n">values</span> <span class="o">*</span> <span class="n">units_per_pixel</span><span class="o">**</span><span class="mi">2</span>
</pre></div>
</div>
</section>
<section id="saving-results">
<h2>Saving results<a class="headerlink" href="#saving-results" title="Link to this heading">¶</a></h2>
<p>Save the grain data and masks:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span><span class="w"> </span><span class="nn">cv2</span>

<span class="c1"># Save grain data to CSV</span>
<span class="n">grain_data</span><span class="o">.</span><span class="n">to_csv</span><span class="p">(</span><span class="n">fname</span><span class="p">[:</span><span class="o">-</span><span class="mi">4</span><span class="p">]</span> <span class="o">+</span> <span class="s2">&quot;.csv&quot;</span><span class="p">)</span>

<span class="c1"># Save mask as PNG</span>
<span class="n">cv2</span><span class="o">.</span><span class="n">imwrite</span><span class="p">(</span><span class="n">fname</span><span class="p">[:</span><span class="o">-</span><span class="mi">4</span><span class="p">]</span> <span class="o">+</span> <span class="s2">&quot;_mask.png&quot;</span><span class="p">,</span> <span class="n">mask_all</span><span class="p">)</span>

<span class="c1"># Save processed image as PNG</span>
<span class="n">cv2</span><span class="o">.</span><span class="n">imwrite</span><span class="p">(</span><span class="n">fname</span><span class="p">[:</span><span class="o">-</span><span class="mi">4</span><span class="p">]</span> <span class="o">+</span> <span class="s2">&quot;_image.png&quot;</span><span class="p">,</span> <span class="n">cv2</span><span class="o">.</span><span class="n">cvtColor</span><span class="p">(</span><span class="n">image</span><span class="p">,</span> <span class="n">cv2</span><span class="o">.</span><span class="n">COLOR_BGR2RGB</span><span class="p">))</span>
</pre></div>
</div>
</section>
<section id="large-image-processing">
<h2>Large image processing<a class="headerlink" href="#large-image-processing" title="Link to this heading">¶</a></h2>
<p>If you want to detect grains in large images, you should use the <code class="docutils literal notranslate"><span class="pre">predict_large_image</span></code> function, which will split the image into patches and run the Unet and SAM segmentations on each patch:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span><span class="w"> </span><span class="nn">PIL</span><span class="w"> </span><span class="kn">import</span> <span class="n">Image</span>
<span class="n">Image</span><span class="o">.</span><span class="n">MAX_IMAGE_PIXELS</span> <span class="o">=</span> <span class="kc">None</span>  <span class="c1"># needed for very large images</span>

<span class="n">all_grains</span><span class="p">,</span> <span class="n">image_pred</span><span class="p">,</span> <span class="n">all_coords</span> <span class="o">=</span> <span class="n">seg</span><span class="o">.</span><span class="n">predict_large_image</span><span class="p">(</span>
    <span class="n">fname</span><span class="p">,</span> <span class="n">model</span><span class="p">,</span> <span class="n">sam</span><span class="p">,</span> <span class="n">min_area</span><span class="o">=</span><span class="mf">400.0</span><span class="p">,</span> <span class="n">patch_size</span><span class="o">=</span><span class="mi">2000</span><span class="p">,</span> <span class="n">overlap</span><span class="o">=</span><span class="mi">200</span>
<span class="p">)</span>
</pre></div>
</div>
<p>Just like before, the <code class="docutils literal notranslate"><span class="pre">all_grains</span></code> list contains shapely polygons of the grains detected in the image. The image containing the grain labels can be generated like this:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">labels</span> <span class="o">=</span> <span class="n">seg</span><span class="o">.</span><span class="n">rasterize_grains</span><span class="p">(</span><span class="n">all_grains</span><span class="p">,</span> <span class="n">large_image</span><span class="p">)</span>
</pre></div>
</div>
<p>See the <a class="reference external" href="https://github.com/zsylvester/segmenteverygrain/blob/main/segmenteverygrain/Segment_every_grain.ipynb">Segment_every_grain.ipynb</a> notebook for a complete example
of how the models can be loaded and used for segmenting an image and QC-ing the result. The notebook goes through all the steps described above in an interactive format.</p>
</section>
</section>
<section id="finetuning-the-u-net-model">
<h1>Finetuning the U-Net model<a class="headerlink" href="#finetuning-the-u-net-model" title="Link to this heading">¶</a></h1>
<p>The last section of the <a class="reference external" href="https://github.com/zsylvester/segmenteverygrain/blob/main/segmenteverygrain/Segment_every_grain.ipynb">Segment_every_grain.ipynb</a> notebook shows how to finetune the U-Net model. The first step is to create patches (usually 256x256 pixels in size) from the images and the corresponding masks that you want to use for training.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">image_dir</span><span class="p">,</span> <span class="n">mask_dir</span> <span class="o">=</span> <span class="n">seg</span><span class="o">.</span><span class="n">patchify_training_data</span><span class="p">(</span><span class="n">input_dir</span><span class="p">,</span> <span class="n">patch_dir</span><span class="p">)</span>
</pre></div>
</div>
<p>The <code class="docutils literal notranslate"><span class="pre">input_dir</span></code> should contain the images and masks that you want to use for training. These files should have ‘image’ and ‘mask’ in their filenames, for example, ‘sample1_image.png’ and ‘sample1_mask.png’. An example image can be found <a class="reference external" href="https://github.com/zsylvester/segmenteverygrain/blob/main/torrey_pines_beach_image.jpeg">here</a>; and the corresponding mask is <a class="reference external" href="https://github.com/zsylvester/segmenteverygrain/blob/main/torrey_pines_beach_mask.png">here</a>.</p>
<p>The mask is an 8-bit image and should contain only three numbers: 0, 1, and 2. 0 is the background, 1 is the grain, and 2 is the grain boundary. Usually the mask is generated using the <code class="docutils literal notranslate"><span class="pre">segmenteverygrain</span></code> workflow, that is, by running the U-Net segmentation first, the SAM segmentation second, and then cleaning up the result. That said, when the U-Net ouputs are of low quality, it might be a good idea to generate the masks directly with SAM. Once you have a good mask, you can save it using <code class="docutils literal notranslate"><span class="pre">cv2.imwrite</span></code> (see also the example notebook):</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">cv2</span><span class="o">.</span><span class="n">imwrite</span><span class="p">(</span><span class="s1">&#39;sample1_mask.png&#39;</span><span class="p">,</span> <span class="n">mask</span><span class="p">)</span>
</pre></div>
</div>
<p>The <code class="docutils literal notranslate"><span class="pre">patch_dir</span></code> is the directory where the patches will be saved. A folder named ‘Patches’ will be created in this directory, and the patches will be saved in subfolders named ‘images’ and ‘labels’.</p>
<p>Next, training, validation, and test datasets are created from the patches:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">train_dataset</span><span class="p">,</span> <span class="n">val_dataset</span><span class="p">,</span> <span class="n">test_dataset</span> <span class="o">=</span> <span class="n">seg</span><span class="o">.</span><span class="n">create_train_val_test_data</span><span class="p">(</span><span class="n">image_dir</span><span class="p">,</span> <span class="n">mask_dir</span><span class="p">,</span> <span class="n">augmentation</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
</pre></div>
</div>
<p>Now we are ready to load the existing model weights and to train the model:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">model</span> <span class="o">=</span> <span class="n">seg</span><span class="o">.</span><span class="n">create_and_train_model</span><span class="p">(</span><span class="n">train_dataset</span><span class="p">,</span> <span class="n">val_dataset</span><span class="p">,</span> <span class="n">test_dataset</span><span class="p">,</span> <span class="n">model_file</span><span class="o">=</span><span class="s1">&#39;seg_model.keras&#39;</span><span class="p">,</span> <span class="n">epochs</span><span class="o">=</span><span class="mi">100</span><span class="p">)</span>
</pre></div>
</div>
<p>If you are happy with the finetuned model, you will want to save it:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">model</span><span class="o">.</span><span class="n">save</span><span class="p">(</span><span class="s1">&#39;seg_model_finetuned.keras&#39;</span><span class="p">)</span>
</pre></div>
</div>
<p>If you want to use this new model to make predictions, you will need to load it with the custom loss function:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">model</span> <span class="o">=</span> <span class="n">load_model</span><span class="p">(</span><span class="s2">&quot;seg_model_finetuned.keras&quot;</span><span class="p">,</span> <span class="n">custom_objects</span><span class="o">=</span><span class="p">{</span><span class="s1">&#39;weighted_crossentropy&#39;</span><span class="p">:</span> <span class="n">seg</span><span class="o">.</span><span class="n">weighted_crossentropy</span><span class="p">})</span>
</pre></div>
</div>
</section>


          </div>
          
        </div>
      </div>
      <div class="sphinxsidebar" role="navigation" aria-label="Main">
        <div class="sphinxsidebarwrapper">
<h1 class="logo"><a href="index.html">segmenteverygrain</a></h1>








<h3>Navigation</h3>
<ul class="current">
<li class="toctree-l1 current"><a class="current reference internal" href="#">Getting started</a><ul>
<li class="toctree-l2"><a class="reference internal" href="#loading-the-models">Loading the models</a></li>
<li class="toctree-l2"><a class="reference internal" href="#running-the-segmentation">Running the segmentation</a></li>
<li class="toctree-l2"><a class="reference internal" href="#quality-control-of-u-net-prediction">Quality control of U-Net prediction</a></li>
<li class="toctree-l2"><a class="reference internal" href="#sam-segmentation">SAM segmentation</a></li>
<li class="toctree-l2"><a class="reference internal" href="#interactive-editing-of-results">Interactive editing of results</a></li>
<li class="toctree-l2"><a class="reference internal" href="#adding-new-grains">Adding new grains</a></li>
<li class="toctree-l2"><a class="reference internal" href="#grain-size-analysis">Grain size analysis</a></li>
<li class="toctree-l2"><a class="reference internal" href="#saving-results">Saving results</a></li>
<li class="toctree-l2"><a class="reference internal" href="#large-image-processing">Large image processing</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="#finetuning-the-u-net-model">Finetuning the U-Net model</a></li>
<li class="toctree-l1"><a class="reference internal" href="api_reference.html">API Reference</a></li>
</ul>

<div class="relations">
<h3>Related Topics</h3>
<ul>
  <li><a href="index.html">Documentation overview</a><ul>
      <li>Previous: <a href="index.html" title="previous chapter">segmenteverygrain</a></li>
      <li>Next: <a href="api_reference.html" title="next chapter">API Reference</a></li>
  </ul></li>
</ul>
</div>
<search id="searchbox" style="display: none" role="search">
  <h3 id="searchlabel">Quick search</h3>
    <div class="searchformwrapper">
    <form class="search" action="search.html" method="get">
      <input type="text" name="q" aria-labelledby="searchlabel" autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false"/>
      <input type="submit" value="Go" />
    </form>
    </div>
</search>
<script>document.getElementById('searchbox').style.display = "block"</script>








        </div>
      </div>
      <div class="clearer"></div>
    </div>
    <div class="footer">
      &#169;2024, Zoltan Sylvester.
      
      |
      Powered by <a href="https://www.sphinx-doc.org/">Sphinx 7.4.7</a>
      &amp; <a href="https://alabaster.readthedocs.io">Alabaster 0.7.16</a>
      
      |
      <a href="_sources/getting_started.rst.txt"
          rel="nofollow">Page source</a>
    </div>

    

    
  </body>
</html>