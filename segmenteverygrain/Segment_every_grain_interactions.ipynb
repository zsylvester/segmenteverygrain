{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "95a8f02b",
   "metadata": {},
   "source": [
    "## Import packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2ebc518",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-01-24T02:02:13.455303Z",
     "iopub.status.busy": "2025-01-24T02:02:13.454797Z",
     "iopub.status.idle": "2025-01-24T02:02:13.467341Z",
     "shell.execute_reply": "2025-01-24T02:02:13.466505Z",
     "shell.execute_reply.started": "2025-01-24T02:02:13.455259Z"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import cv2\n",
    "from skimage.measure import regionprops, regionprops_table\n",
    "from keras.utils import load_img\n",
    "from keras.saving import load_model\n",
    "from importlib import reload\n",
    "import segmenteverygrain as seg\n",
    "import interactions as segi\n",
    "from segment_anything import sam_model_registry, SamPredictor\n",
    "from tqdm import trange, tqdm\n",
    "%matplotlib qt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f474ac20",
   "metadata": {},
   "source": [
    "## Load models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe1bd2bc",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-01-24T02:02:15.162313Z",
     "iopub.status.busy": "2025-01-24T02:02:15.162156Z",
     "iopub.status.idle": "2025-01-24T02:02:17.772801Z",
     "shell.execute_reply": "2025-01-24T02:02:17.772432Z",
     "shell.execute_reply.started": "2025-01-24T02:02:15.162300Z"
    }
   },
   "outputs": [],
   "source": [
    "model = load_model(\"seg_model.keras\", custom_objects={'weighted_crossentropy': seg.weighted_crossentropy})\n",
    "\n",
    "# the SAM model checkpoints can be downloaded from: https://dl.fbaipublicfiles.com/segment_anything/sam_vit_h_4b8939.pth\n",
    "sam = sam_model_registry[\"default\"](checkpoint=\"../sam_vit_h_4b8939.pth\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3c1dfc6",
   "metadata": {},
   "source": [
    "## Run segmentation\n",
    "\n",
    "Grains are supposed to be well defined in the image; e.g., if a grain consists of only a few pixels, it is unlikely to be detected.\n",
    "\n",
    "The segmentation can take a few minutes even for medium-sized images. Images with ~2000 pixels along their largest dimension are a good start and allow the user to get an idea about how well the segmentation works.\n",
    "\n",
    "If you have a much larger image, see the section **\"Run segmentation on large image\"** at the end of the notebook. Running the `predict_large_image` function takes a lot longer (e.g., several hours), but it is possible to analyze very large images with tens of thousands of grains.\n",
    "\n",
    "Image used below is available from [here](https://github.com/zsylvester/segmenteverygrain/blob/main/torrey_pines_beach_image.jpeg)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94bf3f94",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-01-24T02:03:31.981893Z",
     "iopub.status.busy": "2025-01-24T02:03:31.981476Z",
     "iopub.status.idle": "2025-01-24T02:03:35.515711Z",
     "shell.execute_reply": "2025-01-24T02:03:35.515383Z",
     "shell.execute_reply.started": "2025-01-24T02:03:31.981864Z"
    }
   },
   "outputs": [],
   "source": [
    "# replace this with the path to your image:\n",
    "fname = '../torrey_pines_beach_image.jpeg'\n",
    "\n",
    "image = np.array(load_img(fname))\n",
    "image_pred = seg.predict_image(image, model, I=256)\n",
    "\n",
    "# decreasing the 'dbs_max_dist' parameter results in more SAM prompts (and longer processing times):\n",
    "labels, coords = seg.label_grains(image, image_pred, dbs_max_dist=20.0) # Unet prediction"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c091d4c",
   "metadata": {},
   "source": [
    "Use the figure created in the next cell to check the quality of the Unet labeling (sometimes it doesn't work at all) and the distribution of SAM prompts (= black dots). If the Unet prediction is of poor quality, it is a good idea to create some training data and fine tune the base model so that it works better with the images of interest."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e031a13f",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-01-24T02:03:45.748777Z",
     "iopub.status.busy": "2025-01-24T02:03:45.748389Z",
     "iopub.status.idle": "2025-01-24T02:03:45.798745Z",
     "shell.execute_reply": "2025-01-24T02:03:45.798264Z",
     "shell.execute_reply.started": "2025-01-24T02:03:45.748746Z"
    }
   },
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(15,10))\n",
    "ax.imshow(image_pred)\n",
    "plt.scatter(np.array(coords)[:,0], np.array(coords)[:,1], c='k')\n",
    "plt.xticks([])\n",
    "plt.yticks([]);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99bcedac-6a3f-4f1d-97a9-b167fa14994c",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-01-24T02:03:49.103280Z",
     "iopub.status.busy": "2025-01-24T02:03:49.102880Z",
     "iopub.status.idle": "2025-01-24T02:04:48.394323Z",
     "shell.execute_reply": "2025-01-24T02:04:48.393986Z",
     "shell.execute_reply.started": "2025-01-24T02:03:49.103248Z"
    }
   },
   "outputs": [],
   "source": [
    "# SAM segmentation, using the point prompts from the Unet:\n",
    "all_grains, labels, mask_all, grain_data, fig, ax = seg.sam_segmentation(sam, image, image_pred, \n",
    "            coords, labels, min_area=400.0, plot_image=True, remove_edge_grains=False, remove_large_objects=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "421c4444",
   "metadata": {},
   "source": [
    "### Interactive editing\n",
    "\n",
    "The editing interface itself is defined in segmentanything.interactions.\n",
    "\n",
    "Navigation within the interface is described in the [matplotlib documentation](https://matplotlib.org/stable/users/explain/figure/interactive.html#interactive-navigation). Additional controls are:\n",
    "\n",
    "- `Left click`: Select/unselect existing grain or place foreground prompt for grain detection\n",
    "- `Shift + left click/drag`: Create or adjust box prompt for grain detection\n",
    "- `Right click`: Place background prompt for grain detection\n",
    "- `Middle click`: Display measurement information about the indicated grain\n",
    "- `Middle click + drag`: Measure scale bar to calibrate pixels per meter\n",
    "- `Control`: Hold to temporarily hide selected grains\n",
    "- `Escape`: Remove all prompts and unselect all grains\n",
    "- `c`: Use selection box and/or foreground/background prompts to detect a grain\n",
    "- `d`: Delete selected (highlighted) grains\n",
    "- `m`: Merge selected grains (must be touching)\n",
    "- `z`: Delete the most recently-created grain\n",
    "\n",
    "`px_per_m`: The ratio of pixels to meters, if known. This will be overwritten if a scale bar is measured in the interface using middle click & drag.\n",
    "\n",
    "`scale_m`: The length in meters of a reference object. Once the reference object is measured using middle click & drag, size/area values will be converted to meters. The diagonal of the selection box will be taken to represent `scale_m` meters.\n",
    "\n",
    "`image_max_size` (y, x): Images larger than this in either dimension will be downscaled for display. Operations like grain detection will still be performed on the full image, but the display will not be able to zoom in at full quality. This is a performance optimization. Reduce this size for better performance, increase this size for better visual quality when zoomed.\n",
    "\n",
    "`image_alpha`: Set this to a value lower than 1 to apply a fade effect to the background image."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5660dd68",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create Grain objects from detected polygons, providing easy measurement and display methods\n",
    "grains = segi.polygons_to_grains(all_grains, image=image)\n",
    "\n",
    "# Prepare predictor for detecting new grains\n",
    "predictor = SamPredictor(sam)\n",
    "predictor.set_image(image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e0f7f02",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-18T18:07:24.133192Z",
     "iopub.status.busy": "2024-12-18T18:07:24.132645Z",
     "iopub.status.idle": "2024-12-18T18:07:25.415541Z",
     "shell.execute_reply": "2024-12-18T18:07:25.415222Z",
     "shell.execute_reply.started": "2024-12-18T18:07:24.133157Z"
    }
   },
   "outputs": [],
   "source": [
    "# Display interactive interface\n",
    "plot = segi.GrainPlot(\n",
    "    grains,\n",
    "    image = image, \n",
    "    predictor = predictor,\n",
    "    figsize = (12, 8),              # in\n",
    "    px_per_m = 3390.,               # px/m\n",
    "    scale_m = 0.5,                  # m\n",
    "    # image_max_size = (240, 320),  # px\n",
    "    # image_alpha = 1.\n",
    ")\n",
    "\n",
    "# Turn on interactive features (grain editing, etc)\n",
    "plot.activate()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "163eabed",
   "metadata": {},
   "source": [
    "### Results\n",
    "\n",
    "Once manual editing is complete, close the plot and run the following two cells.\n",
    "\n",
    "These results are saved to the location specified in `out_fn`:\n",
    "- Grain shapes, for use elsewhere (geojson)\n",
    "- Image with colorized grains and major/minor axes drawn in (jpg)\n",
    "- Summary data, presenting measurements for each detected grain (csv)\n",
    "- Summary histogram, representing major/minor axes of detected grains (jpg)\n",
    "- Mask representations of the detected grains, in both computer-readable (png, 0-1) and human-readable (jpg, 0-255) formats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02386a92",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-18T18:08:01.061958Z",
     "iopub.status.busy": "2024-12-18T18:08:01.061409Z",
     "iopub.status.idle": "2024-12-18T18:08:01.066268Z",
     "shell.execute_reply": "2024-12-18T18:08:01.065589Z",
     "shell.execute_reply.started": "2024-12-18T18:08:01.061922Z"
    }
   },
   "outputs": [],
   "source": [
    "# Turn off interactive features\n",
    "plot.deactivate()\n",
    "\n",
    "# Draw the major and minor axes onto each grain\n",
    "plot.draw_axes()\n",
    "\n",
    "# Retrieve unit conversion factor\n",
    "px_per_m = plot.px_per_m"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8663eef",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-15T21:30:19.433084Z",
     "iopub.status.busy": "2024-12-15T21:30:19.432525Z",
     "iopub.status.idle": "2024-12-15T21:30:19.519229Z",
     "shell.execute_reply": "2024-12-15T21:30:19.518066Z",
     "shell.execute_reply.started": "2024-12-15T21:30:19.433047Z"
    }
   },
   "outputs": [],
   "source": [
    "# Save results\n",
    "out_fn = '../torrey_pines'\n",
    "# Grain shapes\n",
    "segi.save_grains(out_fn + '_grains.geojson', grains)\n",
    "# Grain image\n",
    "plot.savefig(out_fn + '_grains.jpg')\n",
    "# Summary data\n",
    "summary = segi.save_summary(\n",
    "    out_fn + '_summary.csv', grains, px_per_m=px_per_m)\n",
    "summary.head()\n",
    "# Summary histogram\n",
    "segi.save_histogram(out_fn + '_summary.jpg', summary=summary)\n",
    "# Training mask\n",
    "segi.save_mask(out_fn + '_mask.png', grains, image, scale=False)  # For training\n",
    "segi.save_mask(out_fn + '_mask2.jpg', grains, image, scale=True)  # For viewing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77db7f2b-7718-4466-a1d8-41aa3d3d0803",
   "metadata": {},
   "source": [
    "## Run segmentation on large image (new!)\n",
    "In this case 'fname' points to an image that is larger than a few megapixels and has thousands of grains.\n",
    "The 'predict_large_image' function breaks the input image into smaller patches and it runs the segmentation process on each patch.\n",
    "\n",
    "The image used below (from [Mair et al., 2022, Earth Surface Dynamics](https://esurf.copernicus.org/articles/10/953/2022/)) is available [here](https://github.com/zsylvester/segmenteverygrain/blob/main/mair_et_al_L2_DJI_0382_image.jpg)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90eed72b-4cad-409a-b7bf-bbad0e5dfa74",
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "Image.MAX_IMAGE_PIXELS = None # needed if working with very large images\n",
    "fname = \"mair_et_al_L2_DJI_0382_image.jpg\"\n",
    "all_grains, image_pred, all_coords = seg.predict_large_image(fname, model, sam, min_area=400.0, patch_size=2000, overlap=200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1e52b04-e54e-44e6-8061-b8d0c0c76970",
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot results\n",
    "image = np.array(load_img(fname))\n",
    "fig, ax = plt.subplots(figsize=(15,10))\n",
    "plt.xticks([])\n",
    "plt.yticks([])\n",
    "seg.plot_image_w_colorful_grains(image, all_grains, ax, cmap='Paired')\n",
    "plt.axis('equal')\n",
    "plt.xlim([0, np.shape(image)[1]])\n",
    "plt.ylim([np.shape(image)[0], 0]);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4dae73a8-f9ce-4fb9-8661-19d4c03340f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# this is a faster way of deleting false positives (because it avoids highlighting and deleting the 'bad' grains)\n",
    "grain_inds = []\n",
    "cid1 = fig.canvas.mpl_connect('button_press_event', lambda event: seg.onclick2(event, all_grains, grain_inds, ax=ax, select_only=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98b9650b-c6d9-4c23-85cc-b0234f3d2b2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# delete polygons from 'all_grains'\n",
    "grain_inds = np.unique(grain_inds)\n",
    "grain_inds = sorted(grain_inds, reverse=True)\n",
    "for ind in tqdm(grain_inds):\n",
    "    all_grains.remove(all_grains[ind])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74b9e579-8c81-4baf-9262-eb4b1584c646",
   "metadata": {},
   "source": [
    "After plotting the results, you will want to use the functions for deleting, merging, and adding grains (see above), before saving the results (same workflow as for a small image)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d25703e4-2a4e-4e98-aa49-ff9b816b2ec0",
   "metadata": {},
   "source": [
    "### Finetuning the base model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "396f62c8-be0f-47fe-b660-2c55e2d51d42",
   "metadata": {},
   "outputs": [],
   "source": [
    "# patchify images and masks\n",
    "input_dir = \"./Masks_and_images/\" # the input directory should contain files with 'image' and 'mask' in their filenames\n",
    "patch_dir = \"./New_project/\" # a directory called \"Patches\" will be created here\n",
    "image_dir, mask_dir = seg.patchify_training_data(input_dir, patch_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05f98137-2dfd-4b88-8dc5-e4e79fa343ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create training, validation, and test datasets\n",
    "train_dataset, val_dataset, test_dataset = seg.create_train_val_test_data(image_dir, mask_dir, augmentation=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43650ad9-c2e5-4fc2-856f-4f39b8f1d9e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load base model weights and train the model with the new data\n",
    "model = seg.create_and_train_model(train_dataset, val_dataset, test_dataset, model_file='seg_model.keras', epochs=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41375c1f-5f16-496f-b5e8-497626eb89dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# save finetuned model as new model (this then can be loaded using \"model = load_model(\"new_model.keras\", custom_objects={'weighted_crossentropy': seg.weighted_crossentropy})\"\n",
    "model.save('new_model.keras')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "segmenteverygrain",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
